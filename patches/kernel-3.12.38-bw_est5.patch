diff --exclude CVS --exclude .git -uNr linux-3.12.38/include/linux/tcp_bw_est.h linux-3.12.38.modified/include/linux/tcp_bw_est.h
--- linux-3.12.38/include/linux/tcp_bw_est.h	2015-07-07 23:02:12.629940985 -0400
+++ linux-3.12.38.modified/include/linux/tcp_bw_est.h	2015-07-07 23:00:25.649936319 -0400
@@ -19,6 +19,11 @@
 #define BW_EST_MIN_FIFO_ENTRIES_TO_ENABLE
 #define BW_EST_CONT_TH_US    150
 
+#define BW_EST_BTL_MIN_INTVL_AVG_WINDOW_SHIFT  7 /* Size is multiple of 2 for average operations speed */
+#define BW_EST_BTL_MIN_INTVL_AVG_WINDOW_SIZE  (1 << BW_EST_BTL_MIN_INTVL_AVG_WINDOW_SHIFT)
+/* Count range before setting min bottleneck */
+#define TCP_BW_BLT_MIN_COUNT_MASK   (BW_EST_BTL_MIN_INTVL_AVG_WINDOW_SIZE - 1)
+
 typedef struct avg_fifo {
     unsigned int rd;
     unsigned int wr;
@@ -138,7 +143,7 @@
     unsigned int first_min_rtt;   /* Minimum rtt for the first packet on the pair */
     unsigned int sec_min_rtt; /* Minimum rtt for the second packet on the pair */
     unsigned int blt_min_intvl;  /* Minimum interval bottleneck */
-
+    unsigned int blt_min_intvl_avg; /* Minimum interval bottleneck average */
 }bw_est_stats_t;
 
 /* Auxiliar variables for M/G/1 queue estimation */
@@ -166,10 +171,13 @@
     pkt_series_t pkt_series [BW_EST_AVG_WINDOW_SIZE];
     intvl_series_t intvl_series[BW_EST_AVG_WINDOW_SIZE];
     unsigned int vars[BW_EST_AVG_WINDOW_SIZE]; /* Variances */
+    unsigned int btl_mins[BW_EST_BTL_MIN_INTVL_AVG_WINDOW_SIZE];
     avg_fifo_t cont_series_fifo;
     avg_fifo_t pkt_series_fifo;
     avg_fifo_t intvl_series_fifo;
     avg_fifo_t avg_svc_var_fifo;
+    unsigned int blt_min_win_count;
+    avg_fifo_t avg_svc_fifo;
     mg1_bw_est_t mg1;
     mm1_bw_est_t mm1;
 }m_bw_est_t;
@@ -224,6 +232,41 @@
     return &(wr_entry[wr]);
 }
 
+/* Circular buffer where oldest entries are over written, new entries are added and
+ * accumulator is adjusted for the added and removed entries
+ * 1 if a new entry was added and the latest entry was removed.
+ *   Otherwise, error
+ * */
+static inline int bw_est_fifo_push_btl_min_intvl (avg_fifo_t *fifo_p,
+      unsigned int min_intvl)
+{
+    unsigned int *array = (unsigned int *)fifo_p->array;
+    if (likely (bw_est_fifo_full(fifo_p))) {
+        /* Remove oldest entry from accumulator.
+         * WARNING: Accumulator must not be negative
+         */
+        if (unlikely (fifo_p->accum < array[fifo_p->rd])) {
+            /* Log error */
+            return 0;
+        } else {
+        	fifo_p->accum -= array[fifo_p->rd];
+            fifo_p->rd = fifo_inc(fifo_p->rd, fifo_p->size);
+        }
+
+    } else {
+        fifo_p->count++;
+    }
+
+    /* Add new value */
+    fifo_p->accum += min_intvl;
+
+    /* Add packet and adjust average */
+    array[fifo_p->wr] = min_intvl;
+
+    fifo_p->wr = fifo_inc(fifo_p->wr, fifo_p->size);
+    return 1;
+}
+
 /* Circular buffer where oldest entries are over written
  * return:  1 if a new entry was added and the latest entry was removed.
  *          Otherwise, error
@@ -342,7 +385,7 @@
             return 0;
         } else {
            fifo_p->accum -= array[fifo_p->rd];
-           fifo_p->rd = fifo_inc(fifo_p->rd, BW_EST_AVG_WINDOW_SIZE);
+           fifo_p->rd = fifo_inc(fifo_p->rd, fifo_p->size);
         }
     } else {
        fifo_p->count++;
@@ -351,7 +394,7 @@
     array[fifo_p->wr] = var;
     /* Add new value */
     fifo_p->accum += var;
-    fifo_p->wr = fifo_inc(fifo_p->wr, BW_EST_AVG_WINDOW_SIZE);
+    fifo_p->wr = fifo_inc(fifo_p->wr, fifo_p->size);
 
     return 1;
 }
diff --exclude CVS --exclude .git -uNr linux-3.12.38/net/ipv4/tcp_bw_est.c linux-3.12.38.modified/net/ipv4/tcp_bw_est.c
--- linux-3.12.38/net/ipv4/tcp_bw_est.c	2015-07-07 23:02:12.633940985 -0400
+++ linux-3.12.38.modified/net/ipv4/tcp_bw_est.c	2015-07-07 23:01:02.109937909 -0400
@@ -81,6 +81,13 @@
           bw_est_p->enabled = 0;
    }
 
+   if (bw_est_fifo_init(&bw_est_p->avg_svc_fifo, bw_est_p->btl_mins,
+		   BW_EST_BTL_MIN_INTVL_AVG_WINDOW_SIZE)) {
+		 printk(KERN_ERR"Error initializing interval bottle link mins. "
+			   "Bw estimator will not be activated\n");
+		 bw_est_p->enabled = 0;
+   }
+
    bw_est_p->enabled = 1;
    bw_est_p->processed = 0;
    ret = 0;
@@ -138,6 +145,17 @@
 #define TCP_BW_EST_UTIL_SCALE_POW2    (1ULL<<(TCP_BW_EST_UTIL_SCALE_SHIFT*2ULL))
 #define TCP_BW_EST_BL_SCALE_SHIFT     6UL
 #define TCP_BW_EST_BL_SCALE           (1UL<<TCP_BW_EST_BL_SCALE_SHIFT)
+#define TCP_BW_EST_BL_SCALE_POW2      (1UL<<TCP_BW_EST_BL_SCALE_SHIFT*2ULL)
+/* Scaling up results to manage redundancy issues on divisions and square roots
+ * equations
+ * */
+#define TCP_BW_EST_MG1_UTIL_SCALE_SHIFT   17ULL
+#define TCP_BW_EST_MG1_UTIL_SCALE         (1ULL<<TCP_BW_EST_MG1_UTIL_SCALE_SHIFT)
+/* Scale to the power of two */
+#define TCP_BW_EST_MG1_UTIL_SCALE_POW2    (1ULL<<(TCP_BW_EST_MG1_UTIL_SCALE_SHIFT*2ULL))
+#define TCP_BW_EST_MG1_SVC_VAR_SHIFT	7ULL
+#define TCP_BW_EST_MG1_SVC_VAR_SCALE	(1ULL<<TCP_BW_EST_MG1_SVC_VAR_SHIFT)
+#define TCP_BW_EST_MG1_SVC_VAR_SCALE_POW2	(1ULL<<(TCP_BW_EST_MG1_SVC_VAR_SHIFT*2))
 
 inline int tcp_bw_utlization_est(struct tcp_sock *tp)
 {
@@ -152,7 +170,7 @@
        case TCP_BW_EST_TYPE_MD1:
            ret = (int)(TCP_BW_EST_UTIL_SCALE * (bl + TCP_BW_EST_BL_SCALE) -
                  ll_sqrt( TCP_BW_EST_UTIL_SCALE_POW2 * ((bl*bl)+
-                      TCP_BW_EST_BL_SCALE *  TCP_BW_EST_BL_SCALE) ));
+                		 TCP_BW_EST_BL_SCALE_POW2) ));
            ret = ret >> TCP_BW_EST_BL_SCALE_SHIFT;
            break;
        case TCP_BW_EST_TYPE_MM1:
@@ -166,24 +184,38 @@
        case TCP_BW_EST_TYPE_MG1: {
 
            /* Round trip and variance are obtained from sender measurements */
-           u32 cont_mean2 = tp->bw_est_stats.cont_mean * tp->bw_est_stats.cont_mean;
-           u64 var = tp->bw_est_stats.svc_var_mean * tp->bw_est_stats.svc_var_mean;
-           u32 divd = var - cont_mean2;
-           u64 sqrt2 = (TCP_BW_EST_UTIL_SCALE_POW2 * 2ULL * bl * var) + tp->m_bw_est.mg1.sqrt_div_res;
+           u32 cont_mean2 = tp->bw_est_stats.blt_min_intvl_avg * tp->bw_est_stats.blt_min_intvl_avg;
+           u64 var = tp->bw_est_stats.svc_var_mean;
+           s32 divd = (s32)TCP_BW_EST_MG1_SVC_VAR_SCALE - (s32)(TCP_BW_EST_MG1_SVC_VAR_SCALE * (s32)var)
+        		   / cont_mean2;
+           u64 sqrt2 = (TCP_BW_EST_MG1_SVC_VAR_SCALE_POW2 * 2ULL * bl * TCP_BW_EST_BL_SCALE * var) +
+        		   tp->m_bw_est.mg1.sqrt_div_res;
            u64 sqrt;
-           s64 div;
+           s64 div, div_res;
 
            /* Handling division by zero. The smaller possible scale is 1  */
            tp->m_bw_est.mg1.sqrt_div_res = __div64_32 (&sqrt2, likely(cont_mean2)? cont_mean2 : 1);
-           sqrt = ll_sqrt ((TCP_BW_EST_UTIL_SCALE_POW2 * (bl * bl + 1ULL)) +
-                 sqrt2);
-           div = ((sqrt - (TCP_BW_EST_UTIL_SCALE * (bl + 1ULL))) * tp->bw_est_stats.cont_mean *
-                 tp->bw_est_stats.cont_mean) + tp->m_bw_est.mg1.div_res;
+           sqrt = ll_sqrt ( TCP_BW_EST_MG1_SVC_VAR_SCALE_POW2 *
+        		   ((bl*bl) + TCP_BW_EST_BL_SCALE_POW2) + sqrt2 );
+
+           div = (s64)((s64)TCP_BW_EST_MG1_UTIL_SCALE * ((s64)TCP_BW_EST_MG1_SVC_VAR_SCALE *
+        		   ((s64)bl + (s64)TCP_BW_EST_BL_SCALE) - sqrt));// + (s64)tp->m_bw_est.mg1.div_res;
            /* Handling division by zero. The smaller possible scale is 1 */
-           tp->m_bw_est.mg1.div_res = __div64_32(&div, likely(divd)? divd : 1);
 
-           /* Hopefully, we have number that does not exceed  TCP_BW_EST_UTIL_SCALE */
-           ret = (int)div;
+           div_res = div_s64_rem(div, likely(divd)? divd : 1, &tp->m_bw_est.mg1.div_res);
+
+           if ((div > 0LL) && (divd < 0) && (div_res < 0LL)) {
+        	   tp->bw_est_stats.cont_delta_hist[2] = (s64)TCP_BW_EST_MG1_SVC_VAR_SCALE *
+            		   ((s64)bl + (s64)TCP_BW_EST_BL_SCALE) - sqrt;
+        	   tp->bw_est_stats.cont_delta_hist[3] = (s64)TCP_BW_EST_MG1_SVC_VAR_SCALE *
+            		   ((s64)bl + (s64)TCP_BW_EST_BL_SCALE);
+        	   tp->bw_est_stats.cont_delta_hist[4] = (s64)bl;
+        	   tp->bw_est_stats.cont_delta_hist[0] = (u64)sqrt & 0xFFFFFFFFULL;
+        	   tp->bw_est_stats.cont_delta_hist[5] = ((u64)sqrt >> 32) & 0xFFFFFFFFULL;
+        	   tp->bw_est_stats.cont_delta_hist[6] = tp->m_bw_est.mg1.div_res;
+           }
+
+           ret = (int)div_res;
            ret = ret >> TCP_BW_EST_BL_SCALE_SHIFT;
        }
 
@@ -213,15 +245,25 @@
 {
     const int util = tcp_bw_utlization_est(tp);
     u64 bdpe_64; /* Estimated bw product */
+    u64 scaler, shift;
 
     if (unlikely(util < 0)) {
+    	tp->bw_est_stats.cont_delta_hist[1] = util;
         tp->bw_est_stats.err_util++;
         return -1;
     }
 
+    if (tp->bw_est_stats.est_mode == TCP_BW_EST_TYPE_MG1) {
+    	scaler = TCP_BW_EST_MG1_UTIL_SCALE_SHIFT;
+    	shift = TCP_BW_EST_MG1_UTIL_SCALE_SHIFT;
+    } else {
+    	scaler = TCP_BW_EST_UTIL_SCALE;
+    	shift = TCP_BW_EST_UTIL_SCALE_SHIFT;
+    }
+
     /* scale * link (1 - p) = link ( scale - (scale * p) ) */
-    bdpe_64 = tp->bw_est_stats.link_capacity * (TCP_BW_EST_UTIL_SCALE - util);
-    bdpe_64 = ((bdpe_64 >> TCP_BW_EST_UTIL_SCALE_SHIFT) *
+    bdpe_64 = tp->bw_est_stats.link_capacity * (scaler - util);
+    bdpe_64 = ((bdpe_64 >> shift) *
           (jiffies_to_usecs(tp->srtt)>>3)) + tp->bw_est_stats.bdpe_res;
     tp->bw_est_stats.bdpe_res = __div64_32 (&bdpe_64,
     		tp->mss_cache * USEC_PER_SEC /* Normalizing rtt */);
@@ -360,6 +402,129 @@
 }
 
 /*
+ * Calculates average bottle neck link capacity
+ * @return: - 0 - Success calculating capacity. Otherwise,
+ * 			Error
+ */
+static inline int tcp_bw_est_calc_avg_capacity (struct tcp_sock *tp)
+{
+	unsigned int count = tp->m_bw_est.avg_svc_fifo.count;
+	unsigned int accum = tp->m_bw_est.avg_svc_fifo.accum;
+	if (likely(count)) {
+		tp->bw_est_stats.blt_min_intvl_avg = accum / count;
+		//tp->bw_est_stats.cont_mean_res = accum % count;
+	} else {
+		tp->bw_est_stats.blt_min_intvl_avg = accum;
+//		tp->bw_est_stats.cont_mean_res = 0;
+	}
+
+	{
+	u64 link_capacity = ((u64)tp->mss_cache * (u64)USEC_PER_SEC) +
+		  (u64)tp->bw_est_stats.link_capacity_res;
+
+	u32 blt_min_intvl = tp->bw_est_stats.blt_min_intvl_avg;
+	tp->bw_est_stats.link_capacity_res =
+		  __div64_32(&link_capacity, likely(blt_min_intvl)? blt_min_intvl : 1);
+	tp->bw_est_stats.link_capacity = link_capacity;
+	}
+
+	return 0;
+}
+
+/*
+ * Estimate average bottleneck minimum interval between packets. The inverse of this values
+ * is the bottle neck capacity
+ * @return - 0 If success processing bottleneck minimum interval. Otherwise
+ * 			 Error
+ * Note: Function arguments sanity are not checked for performance issues
+ */
+static inline int tcp_bw_est_avg_btl_min_rx_intvl (struct tcp_sock *tp,
+		cont_series_t *cont_first_p, pkt_series_t *last_pkt_series_p,
+		unsigned int rx_delta, unsigned int rtt)
+{
+	int ret = -1;  /* Assume error finding min interval */
+
+	unsigned int first_rtt = 0, second_rtt = 0;
+	unsigned char no_error = 0;  /*Assume error */
+	unsigned int old_first_rtt = tp->bw_est_stats.first_min_rtt;
+	unsigned int old_second_rtt = tp->bw_est_stats.sec_min_rtt;
+
+	/* Find if packet is first on the pair */
+	if (cont_first_p->first) {
+		if (unlikely (!old_first_rtt)) {
+			old_first_rtt = tp->bw_est_stats.first_min_rtt = cont_first_p->rtt;
+		}
+		if (unlikely (!old_second_rtt)) {
+			old_second_rtt = tp->bw_est_stats.sec_min_rtt = rtt;
+		}
+		first_rtt = cont_first_p->rtt;
+		second_rtt = rtt;
+		no_error = 1;
+
+	} else if (last_pkt_series_p->pushed_cont) {
+		/* If did not see the first packet on a packets train but
+		 * the packet is described as a first in a packet pair
+		 * (A second packets follows it)
+		 */
+		if (unlikely (!old_first_rtt)){
+			old_first_rtt = tp->bw_est_stats.first_min_rtt = last_pkt_series_p->rtt;
+		}
+		if (unlikely (!old_second_rtt)) {
+			old_second_rtt = tp->bw_est_stats.sec_min_rtt = rtt;
+		}
+		first_rtt = last_pkt_series_p->rtt;
+		second_rtt = rtt;
+		no_error = 1;
+	} else {
+		/* Error, not sure how you get here. Log it */
+		return ret;
+	}
+
+	if (likely(no_error)) {
+//		char min_changed = 0;   /* Checks if new min has been found */
+		/* Compare min sum with current sum plus margin of error */
+
+		if ((first_rtt + second_rtt) < (old_first_rtt + old_second_rtt)) {
+			/* If min is true, get capacity */
+			tp->bw_est_stats.blt_min_intvl = rx_delta;
+		}
+
+		/* Update for min in first and second if required */
+		if (first_rtt < old_first_rtt) {
+			tp->bw_est_stats.first_min_rtt = first_rtt;
+			/* Update instantaneous first min rtt */
+		}
+		if (second_rtt < old_second_rtt) {
+			tp->bw_est_stats.sec_min_rtt = second_rtt;
+			/* Update instantaneous second min rtt */
+		}
+		/* Every time counter reaches instantaneous bottle neck min
+		 * interval, reset min interval counters and values.
+		 * Find new average for total connection time
+		 * */
+		if (!(tp->m_bw_est.blt_min_win_count & TCP_BW_BLT_MIN_COUNT_MASK)) {
+			/* Add to fifo */
+			if (!bw_est_fifo_push_btl_min_intvl(&tp->m_bw_est.avg_svc_fifo,
+					tp->bw_est_stats.blt_min_intvl)) {
+				return ret;
+			}
+			tcp_bw_est_calc_avg_capacity(tp);
+
+			if (unlikely (tcp_bw_est_calc_var (tp,
+					tp->bw_est_stats.blt_min_intvl - tp->bw_est_stats.blt_min_intvl_avg))) {
+				return -1;
+			}
+			/* Clear results of current calculations */
+			tp->bw_est_stats.first_min_rtt = tp->bw_est_stats.sec_min_rtt =
+					tp->bw_est_stats.blt_min_intvl = 0;
+		}
+		tp->m_bw_est.blt_min_win_count++;
+	}
+
+	return 0;
+}
+
+/*
  * Estimate bottleneck minimum interval between packets. The inverse of this values
  * is the bottle neck capacity
  * @return - 0 If success processing bottleneck minimum interval. Otherwise
@@ -495,7 +660,6 @@
 				}
 				last_pkt_series_p->pushed_cont = 1;
 			}
-			tp->bw_est_stats.cont_delta_hist[0]++;
 			// Now pushed current packet series
 			cont_series.tx_delta = (unsigned int)((signed int)pkt_series.sent -
 							(signed int)last_pkt_series_p->sent);
@@ -516,12 +680,23 @@
 //			}
 			pkt_series.pushed_cont = 1;  /* Marked packets as pushed */
 
-			/* Now find capacity by detecting minimum rtt delay */
-			if (tcp_bw_est_find_btl_min_rx_intvl(tp, &cont_first, last_pkt_series_p,
-					cont_series.rx_delta, rtt)) {
-				tp->bw_est_stats.cont_push_err++;
-				bw_est_fifo_push_pkt_series(pkt_series_fifo_p, &pkt_series);
-				return -1;
+			if (likely(tp->bw_est_stats.est_mode == TCP_BW_EST_TYPE_MG1)) {
+				/* Now find capacity by detecting minimum rtt delay */
+				if (unlikely (tcp_bw_est_avg_btl_min_rx_intvl(tp, &cont_first, last_pkt_series_p,
+						cont_series.rx_delta, rtt))) {
+					tp->bw_est_stats.cont_push_err++;
+					bw_est_fifo_push_pkt_series(pkt_series_fifo_p, &pkt_series);
+					return -1;
+				}
+
+			} else {
+				/* Now find capacity by detecting minimum rtt delay */
+				if (tcp_bw_est_find_btl_min_rx_intvl(tp, &cont_first, last_pkt_series_p,
+						cont_series.rx_delta, rtt)) {
+					tp->bw_est_stats.cont_push_err++;
+					bw_est_fifo_push_pkt_series(pkt_series_fifo_p, &pkt_series);
+					return -1;
+				}
 			}
 
 			tcp_bw_est_calc_rx_delta_mean(tp, delta_rx);
